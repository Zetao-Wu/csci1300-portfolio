<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>&lt;Melodic Machines /&gt; | Case Study</title>
  <link rel="stylesheet" href="melodic.css">
  <link href="https://fonts.googleapis.com/css2?family=Share+Tech+Mono&display=swap" rel="stylesheet">
</head>

<body>
  <nav>
    <a href="../" class="logo">&lt;/AccessibleComponent&gt;</a>
    <div class="menu-toggle" id="menu-toggle">
      <span></span>
      <span></span>
      <span></span>
    </div>
    <ul class="nav-links" id="nav-links">
      <a href="../#hero">01. &lt;home&gt;</a>
      <a href="../#skills">02. &lt;skills&gt;</a>
      <a href="../#experience">03. &lt;experience&gt;</a>
      <a href="../#projects">04. &lt;projects&gt;</a>
      <a href="../#contact">05. &lt;contact&gt;</a>
    </ul>
  </nav>

  <main>
    <section class="hero">
      <div class="hero-container">
        <div class="text_hero">
          <h1>Melodic<br>&&<br>Machines</h1>
        </div>
        <div class="image_hero">
          <img src="./melodic.png" alt="melodic machines poster">
        </div>
      </div>
    </section>

    <section class="overview">
      <div class="overview-inner">
        <h2>Overview</h2>
        <p>Melodic Machines is a dual-model generative AI system designed to produce artist-conditioned music outputs by combining two distinct architectures: a Transformer model for lyric generation and a Stable Diffusion model for audio spectrogram synthesis.</p>
        <ul>
          <li><strong>Goal:</strong> Automate music generation while preserving artistic style.</li>
          <li><strong>Architecture:</strong> Lyrics and spectrograms generated independently then paired.</li>
          <li><strong>Innovation:</strong> Uses artist/genre conditioning for stylistic control.</li>
        </ul>
      </div>
    </section>

    <section class="data">
      <h2>Data Pipeline</h2>
      <ul>
        <li>Lyrics Model: Famous artist-lyric pairs from Kaggle, cleaned and split into train/test sets.</li>
        <li>Spectrogram Model: Used FreeMusicArchive (FMA) audio and genre-tagged mp3 files, converted into spectrograms.</li>
      </ul>
      <div class="centered-img">
        <img src="./spectrogram-sample.png" alt="sample spectrogram visualization">
      </div>
    </section>

    <section class="architecture">
      <h2>Model Architectures</h2>
      <div class="dual-grid">
        <div>
          <h3>Transformer</h3>
          <p>Autoregressive decoder-only transformer model trained to predict next token in artist-specific lyrics.</p>
          <img src="./transformer.png" alt="transformer block diagram">
        </div>
        <div>
          <h3>Stable Diffusion</h3>
          <p>Conditional U-Net trained on spectrograms to generate audio representations given artist & genre embeddings.</p>
          <img src="./diffusion.png" alt="diffusion block diagram">
        </div>
      </div>
    </section>

    <section class="results">
      <h2>Results</h2>
      <div class="result-split">
        <div>
          <img src="./lyric-gen.png" alt="sample generated lyrics">
        </div>
        <div>
          <img src="./spectrogram-compare.png" alt="real vs generated spectrograms">
        </div>
      </div>
      <p>Transformer achieves meaningful stylistic similarity; diffusion model shows visually coherent spectrograms, though improvements are needed in fine detail rendering and waveform fidelity.</p>
    </section>

    <section class="ethics">
      <h2>Discussion & Ethics</h2>
      <ul>
        <li><strong>Bias:</strong> Generated content may unintentionally reinforce artist stereotypes.</li>
        <li><strong>Attribution:</strong> Generated media risks misrepresenting real-world creators.</li>
        <li><strong>Training Limitation:</strong> Style modeling quality highly dependent on dataset granularity and balance.</li>
      </ul>
    </section>

    <section class="final-note">
      <h2>Acknowledgments</h2>
      <p>Created as a final project for CSCI 1470: Deep Learning at Brown University. Special thanks to Prof. Eric Ewing and our mentors for their support and feedback on model design and evaluation.</p>
    </section>
  </main>

  <script>
    const toggle = document.getElementById('menu-toggle');
    const links = document.getElementById('nav-links');
    toggle.addEventListener('click', () => links.classList.toggle('active'));
  </script>
</body>

</html>
